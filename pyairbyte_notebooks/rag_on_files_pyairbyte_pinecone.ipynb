{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s3Jow7_BjQi"
      },
      "source": [
        "# End-to-End RAG Tutorial Using Source-File, PyAirbyte, Pinecone, and LangChain\n",
        "\n",
        "This Python notebook demonstrates an end-to-end process for integrating data from a file source into a Pinecone vector store using PyAirbyte. It also showcases a short demo of using OpenAI's LangChain to analyze the data stored in Pinecone with a RAG (Retrieval-Augmented Generation) model.\n",
        "\n",
        "Source File provides the possibility to acquire data from several storage providers, which you can get details of [here](https://docs.airbyte.com/integrations/sources/file#step-2-select-the-provider-and-set-provider-specific-configurations).\n",
        "\n",
        "In this article, we will use S3 as storage, however you can set up your own storage provider.(Checkout [refrences](https://docs.airbyte.com/integrations/sources/file#reference)).\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Storage Provider Account**: Visit [docs](https://docs.airbyte.com/integrations/sources/file#step-2-select-the-provider-and-set-provider-specific-configurations) to obtain the essential information to setup the source .\n",
        "\n",
        "2. **Pinecone Account**: Sign up on [Pinecone](https://www.pinecone.io/) and generate an API key in your project settings as per the [documentation](https://docs.pinecone.io/guides/get-started/quickstart).\n",
        "\n",
        "3. **OpenAI API Key**: Create an account at [OpenAI](https://openai.com/api/), then generate an API key in the API section following the [documentation](https://platform.openai.com/docs/quickstart).\n",
        "\n",
        "## Install PyAirbyte and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# First, we need to install the necessary libraries.\n",
        "!pip3 install airbyte openai langchain pinecone-client langchain-openai langchain-pinecone python-dotenv langchainhub"
      ],
      "metadata": {
        "id": "XclQfDX9MQsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Source File with PyAirbyte\n",
        "\n",
        "Here we will setup the source file with AWS s3 storage. Checkout this [specs](https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-file/source_file/spec.json) for setting up appropriate key-value pair in ab.get_source.\n",
        "\n",
        "Note: The credentials are retrieved securely using the get_secret() method. This will automatically locate a matching Google Colab secret or environment variable, ensuring they are not hard-coded into the notebook. Make sure to add your key to the Secrets section on the left.\n"
      ],
      "metadata": {
        "id": "Hzira-9BQq0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2pgyG5aMGq0"
      },
      "outputs": [],
      "source": [
        "import airbyte as ab\n",
        "\n",
        "source = ab.get_source(\n",
        "    \"source-file\",\n",
        "    config={\n",
        "        \"dataset_name\": \"products\", # The Name of the final table to replicate this file into (should include letters, numbers dash and underscores only).\n",
        "        \"format\": \"csv\", # Supported file format: https://docs.airbyte.com/integrations/sources/file#file-formats\n",
        "        \"url\": ab.get_secret(\"URL\"), # The URL path to access the file which should be replicated.\n",
        "        \"provider\":{\n",
        "            \"storage\": \"S3\", # Change the storage provider\n",
        "            \"aws_access_key_id\": ab.get_secret(\"AWS_ACCESS_KEY\"),\n",
        "            \"aws_secret_access_key\": ab.get_secret(\"AWS_SECRET_KEY\"),\n",
        "        }\n",
        "    }\n",
        ")\n",
        "source.check()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process outlines how to retrieve data from a file using Airbyte and transform it into a suitable format for additional processing or analysis."
      ],
      "metadata": {
        "id": "GO0QI-qNRNyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Phdo7l_MGq2"
      },
      "outputs": [],
      "source": [
        "source.select_all_streams() # Select all streams\n",
        "read_result = source.read() # Read the data\n",
        "product = [doc for value in read_result.values() for doc in value.to_documents()]\n",
        "\n",
        "print(str(product[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Langchain to build a RAG pipeline.\n",
        "\n",
        "The code uses RecursiveCharacterTextSplitter to break documents into smaller chunks. Metadata within these chunks is converted to strings. This facilitates efficient processing of large texts, enhancing analysis capabilities."
      ],
      "metadata": {
        "id": "jJ5Na_O2Sn1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wravAgJhMGq3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.split_documents(product)\n",
        "print(f\"Created {len(chunked_docs)} document chunks.\")\n",
        "\n",
        "for doc in chunked_docs:\n",
        "    for md in doc.metadata:\n",
        "        doc.metadata[md] = str(doc.metadata[md])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tcXR48fsMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "## Embedding Technique Of OPENAI\n",
        "os.environ['OPENAI_API_KEY'] = ab.get_secret(\"OPENAI_API_KEY\")\n",
        "embeddings=OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Pinecone\n",
        "\n",
        "Pinecone is a managed vector database service designed for storing, indexing, and querying high-dimensional vector data efficiently.\n"
      ],
      "metadata": {
        "id": "Mh_lGwiJUkLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXgQF-xMGq4"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "\n",
        "os.environ['PINECONE_API_KEY'] = ab.get_secret(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone()\n",
        "index_name = \"productindex\" # Replace with your index name\n",
        "\n",
        "\n",
        "# # Uncomment this if you have not created pinecone index already\n",
        "# spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\") #Replace with your cloud and region\n",
        "# pc.create_index(\n",
        "#         name=index_name,\n",
        "#         dimension=1536,\n",
        "#         metric='cosine',\n",
        "#         spec=spec\n",
        "# )\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PineconeVectorStore is a class provided by the LangChain library specifically designed for interacting with Pinecone vector stores.\n",
        "from_documents method of PineconeVectorStore is used to create or update vectors in a Pinecone vector store based on the provided documents and their corresponding embeddings."
      ],
      "metadata": {
        "id": "evdVEEsTUwhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nU9KjhMHMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    chunked_docs, embeddings, index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now setting up a pipeline for RAG using LangChain, incorporating document retrieval from Pinecone, prompt configuration, and a chat model from OpenAI for response generation."
      ],
      "metadata": {
        "id": "8zwaoOteU7oi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2e-raMYMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = pinecone.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"Langchain RAG pipeline set up successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihbo8bllMGq4"
      },
      "outputs": [],
      "source": [
        "print(rag_chain.invoke(\"What is the cost of Canon Photo Ink Cartridge - CL52?\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}