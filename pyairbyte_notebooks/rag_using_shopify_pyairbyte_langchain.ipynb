{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s3Jow7_BjQi"
      },
      "source": [
        "# End-to-End RAG Tutorial Using Shopify, PyAirbyte, Pinecone, and LangChain\n",
        "\n",
        "This notebook demonstrates an end-to-end Retrieval-Augmented Generation (RAG) pipeline. We will extract data from an Shopify bucket using PyAirbyte, store it in a Pinecone vector store, and then use LangChain to perform RAG on the stored data. This workflow showcases how to integrate these tools to build a scalable RAG system.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Shopify**:\n",
        "   - Follow the instructions in the [Shopify Connector Docs](https://docs.airbyte.com/integrations/sources/shopify#setup-guide) to set up your Shopify and obtain the necessary access keys.\n",
        "\n",
        "2. **Pinecone Account**:\n",
        "   - **Create a Pinecone Account**: Sign up for an account on the [Pinecone website](https://www.pinecone.io/).\n",
        "   - **Obtain Pinecone API Key**: Generate a new API key from your Pinecone project settings. For detailed instructions, refer to the [Pinecone documentation](https://docs.pinecone.io/docs/quickstart).\n",
        "\n",
        "3. **OpenAI API Key**:\n",
        "   - **Create an OpenAI Account**: Sign up for an acco\n",
        "   unt on [OpenAI](https://www.openai.com/).\n",
        "   - **Generate an API Key**: Go to the API section and generate a new API key. For detailed instructions, refer to the [OpenAI documentation](https://beta.openai.com/docs/quickstart).\n",
        "\n",
        "\n",
        "## Install PyAirbyte and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XclQfDX9MQsw"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# First, we need to install the necessary libraries.\n",
        "!pip3 install airbyte openai langchain pinecone-client langchain-openai langchain-pinecone python-dotenv langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzira-9BQq0h"
      },
      "source": [
        "## Setup Source Shopify with PyAirbyte\n",
        "\n",
        "The provided code configures an Airbyte source to extract data from a Shopify store.\n",
        "\n",
        "To configure according to your requirements, you can refer to [this references](https://docs.airbyte.com/integrations/sources/shopify#reference).\n",
        "\n",
        "Note: The credentials are retrieved securely using the get_secret() method. This will automatically locate a matching Google Colab secret or environment variable, ensuring they are not hard-coded into the notebook. Make sure to add your key to the Secrets section on the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2pgyG5aMGq0",
        "outputId": "f20c47bf-1dcb-42bd-ddbb-a7b1ab939afe"
      },
      "outputs": [],
      "source": [
        "import airbyte as ab\n",
        "\n",
        "source = ab.get_source(\n",
        "    \"source-shopify\",\n",
        "    config={\n",
        "\n",
        "        \"credentials\":{\n",
        "            # There are two methods available for authentication 'api_password' and 'oauth2.0',\n",
        "            # Choose one of them (https://docs.airbyte.com/integrations/sources/shopify#airbyte-open-source)\n",
        "            \"auth_method\": \"api_password\",\n",
        "            \"api_password\": ab.get_secret(\"API_PASSWORD\")\n",
        "        },\n",
        "        \"shop\":ab.get_secret(\"STORE_NAME\"),\n",
        "        \"start_date\": ab.get_secret(\"START_DATE\"),\n",
        "        \"bulk_window_in_days\": 30, # change this according to your requirement (Defines what would be a date range per single BULK Job)\n",
        "        \"fetch_transactions_user_id\": False\n",
        "    }\n",
        ")\n",
        "source.check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO0QI-qNRNyz"
      },
      "source": [
        "This is a basic process of fetching data from an  using Airbyte and converting it into a format suitable for further processing or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Phdo7l_MGq2"
      },
      "outputs": [],
      "source": [
        "# List the available streams available for the Shopify source\n",
        "# Available Stream for shopify here(https://docs.airbyte.com/integrations/sources/shopify#supported-streams)\n",
        "source.get_available_streams()\n",
        "# Select the streams we are interested in loading to cache\n",
        "source.select_streams([\"products\", \"product_variants\", \"collections\", \"customers\"])\n",
        "cache = ab.get_default_cache()\n",
        "result = source.read(cache=cache)\n",
        "\n",
        "product_details = [doc for doc in result[\"products\"].to_documents()]  # Fetching data for products stream only\n",
        "\n",
        "print(str(product_details[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ5Na_O2Sn1U"
      },
      "source": [
        "# Use Langchain to build a RAG pipeline.\n",
        "\n",
        "The code uses RecursiveCharacterTextSplitter to break documents into smaller chunks. Metadata within these chunks is converted to strings. This facilitates efficient processing of large texts, enhancing analysis capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wravAgJhMGq3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.split_documents(product_details)\n",
        "print(f\"Created {len(chunked_docs)} document chunks.\")\n",
        "\n",
        "for doc in chunked_docs:\n",
        "    for md in doc.metadata:\n",
        "        doc.metadata[md] = str(doc.metadata[md])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcXR48fsMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "## Embedding Technique Of OPENAI\n",
        "os.environ['OPENAI_API_KEY'] = ab.get_secret(\"OPENAI_API_KEY\")\n",
        "embeddings=OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh_lGwiJUkLg"
      },
      "source": [
        "## Setting up Pinecone\n",
        "\n",
        "Pinecone is a managed vector database service designed for storing, indexing, and querying high-dimensional vector data efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXgQF-xMGq4"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "\n",
        "os.environ['PINECONE_API_KEY'] = ab.get_secret(\"PINECONE_API_KEY\")\n",
        "pc = Pinecone()\n",
        "index_name = \"shopifyproductsindex\" # Replace with your index name\n",
        "\n",
        "\n",
        "# Uncomment this if you have not created a Pinecone index yet\n",
        "\n",
        "# spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\") #Replace with your cloud and region\n",
        "# pc.create_index(\n",
        "#         name=index_name,\n",
        "#         dimension=1536,\n",
        "#         metric='cosine',\n",
        "#         spec=spec\n",
        "# )\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evdVEEsTUwhE"
      },
      "source": [
        "PineconeVectorStore is a class provided by the LangChain library specifically designed for interacting with Pinecone vector stores.\n",
        "from_documents method of PineconeVectorStore is used to create or update vectors in a Pinecone vector store based on the provided documents and their corresponding embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU9KjhMHMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    chunked_docs, embeddings, index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zwaoOteU7oi"
      },
      "source": [
        "Now setting up a pipeline for RAG using LangChain, incorporating document retrieval from Pinecone, prompt configuration, and a chat model from OpenAI for response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2e-raMYMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = pinecone.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"Langchain RAG pipeline set up successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihbo8bllMGq4"
      },
      "outputs": [],
      "source": [
        "print(rag_chain.invoke(\"What type of products do we sell?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
