{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Illustrating the usage of *langchain_airbyte* package**\n",
        "\n",
        "The `langchain-airbyte` package integrates LangChain with Airbyte.<br>\n",
        "\n",
        "It has a very powerful function  `AirbyteLoader` which can be used to load data as document into langchain from any Airbyte source!<br>\n",
        "\n",
        "This notebook demonstrates the usage of `langchain_airbyte` to load data from an Airbyte source (Github Repository) , store the data into a vector database, and perform a basic QnA on that data using FAISS and OpenAI embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "5eHLkb0FkdXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequisite**\n",
        "**1) OpenAI API Key**:\n",
        "   - **Create an OpenAI Account**: Sign up for an account on [OpenAI](https://www.openai.com/).\n",
        "   - **Generate an API Key**: Go to the API section and generate a new API key. For detailed instructions, refer to the [OpenAI documentation](https://beta.openai.com/docs/quickstart).\n",
        "\n",
        "**2) Github Personal Access Token**:\n",
        "   - **Create a Github Account**: Sign up for an account on [Github](https://www.github.com/).\n",
        "   - **Generate an API Key**: Cick on your profile icon->Settings->Developer Settings and generate a new API key. For detailed instructions, refer to the [Github documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).\n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "xwF4ZyZp2ji0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Dependencies**\n",
        "Lets start by installing all the required dependencies! <br>\n",
        "First of all we will create a virtual environment and then begin installing the dependencies.\n"
      ],
      "metadata": {
        "id": "sC1_LEcYoU1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "#Installing the necessary libraries\n",
        "!pip3.10 install -qU langchain-airbyte faiss-cpu langchain-community langchain-openai"
      ],
      "metadata": {
        "id": "7lwuMXN3ocjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data using AirbyteLoader**\n",
        "Now we use `AirbyteLoader` to fetch data from the source  `source-github`.<br>\n",
        "You may use any other source you require, but fetch the data accordingly!<br>\n",
        "Dont forget to add all the required fields!<br>\n",
        "Refer the guide for your source [here](https://docs.airbyte.com/integrations/sources/)\n",
        "\n",
        "For more information regarding this package [refer](https://python.langchain.com/v0.2/docs/integrations/document_loaders/airbyte/)\n",
        "\n",
        "The last step of converting data to documents ensures that the raw data (GitHub commits) is converted into a standardized format that includes both the main content and any associated metadata."
      ],
      "metadata": {
        "id": "MdmoHC1Fo7fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_airbyte import AirbyteLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Configure the AirbyteLoader to load data from a GitHub repository\n",
        "loader = AirbyteLoader(\n",
        "    source=\"source-github\",\n",
        "    stream=\"commits\",\n",
        "    config={\n",
        "        \"credentials\": {\n",
        "            \"personal_access_token\": \"your_personal_access_token\"\n",
        "        },\n",
        "        \"repositories\": [\"your_username/repository_name\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Load documents from the specified GitHub source\n",
        "docs = loader.load()\n",
        "\n",
        "# Convert incoming stream data into documents\n",
        "docs = [Document(page_content=record.page_content, metadata=record.metadata) for record in docs]\n"
      ],
      "metadata": {
        "id": "q-9PC6bzpQMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split Documents into Chunks and Store these Chunks in Vector Store using FAISS**\n",
        "Large documents are split into smaller chunks to make them easier to handle. This also helps in improving the efficiency of the retrieval process, as smaller chunks can be more relevant to specific queries. <br>\n",
        "\n",
        "The chunks of documents are transformed into vectors using an embedding model (OpenAI embeddings).<br>\n",
        "These vectors are then stored in a FAISS vector store, which allows for efficient similarity search.<br>\n",
        "The vector store indexes the vectors and enables fast retrieval of similar vectors based on a query."
      ],
      "metadata": {
        "id": "61JWR7I_p7g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Created {len(chunked_docs)} document chunks.\")\n",
        "\n",
        "# Store Chunks in Vector Store using FAISS\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import os\n",
        "\n",
        "# Set the OpenAI API Key (make sure to set your own API key here)\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# Ensure filtered_docs is not empty\n",
        "if not chunked_docs:\n",
        "    raise ValueError(\"No valid documents to store in the vector store.\")\n",
        "\n",
        "# Store document chunks in FAISS vector store\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "vector_store = FAISS.from_texts([doc.page_content for doc in chunked_docs], embeddings)\n",
        "\n",
        "print(\"Chunks successfully stored in vectorstore.\")"
      ],
      "metadata": {
        "id": "vqI6yNtGqJC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform QnA on Stored Data**\n",
        "Finally we perform the Question And Answer here.<br>\n",
        "\n",
        "When a query is made, the vector store retrieves relevant document chunks based on their vector similarity to the query.\n",
        "The language model (OpenAI) then generates answers based on the retrieved chunks."
      ],
      "metadata": {
        "id": "xUuF_RUvqVZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Perform QnA on Stored Data\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Initialize the LLM (OpenAI)\n",
        "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create a QnA chain\n",
        "qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
        "\n",
        "# Perform a QnA\n",
        "query = \"What are the latest commits in the repository?\"\n",
        "inputs = {\"question\": query, \"input_documents\": chunked_docs}\n",
        "answer = qa_chain.invoke(inputs)\n",
        "\n",
        "print(\"QnA Result:\", answer)\n"
      ],
      "metadata": {
        "id": "tHDAkPjYqXDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}