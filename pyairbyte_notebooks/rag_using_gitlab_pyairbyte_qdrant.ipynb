{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s3Jow7_BjQi"
      },
      "source": [
        "# End-to-End RAG Tutorial Using Gitlab, PyAirbyte, Qdrant, and LangChain\n",
        "\n",
        "This notebook demonstrates an end-to-end Retrieval-Augmented Generation (RAG) pipeline. We will extract data from an gitlab using PyAirbyte, store it in a qdrantvector store, and then use LangChain to perform RAG on the stored data. This workflow showcases how to integrate these tools to build a scalable RAG system.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Gitlab Account**:\n",
        "   - Follow the instructions in the [Gitlab Docs](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html#create-a-personal-access-token) to set up your gitlab account and obtain the necessary access token.\n",
        "\n",
        "2. **Qdrant Account**:\n",
        "   - **Create a Qdrant Account**: Sign up for an account on the Qdrant [website](https://qdrant.tech/)\n",
        "   - **Create Cluster**: Open the Qdrant dashboard and establish a new cluster. After building a new cluster, you will see an option for creating API_key; copy the URL and API_key from there.\n",
        "\n",
        "3. **OpenAI API Key**:\n",
        "   - **Create an OpenAI Account**: Sign up for an acco\n",
        "   unt on [OpenAI](https://www.openai.com/).\n",
        "   - **Generate an API Key**: Go to the API section and generate a new API key. For detailed instructions, refer to the [OpenAI documentation](https://beta.openai.com/docs/quickstart).\n",
        "\n",
        "\n",
        "## Install PyAirbyte and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XclQfDX9MQsw"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# First, we need to install the necessary libraries.\n",
        "!pip3 install airbyte langchain langchain-openai qdrant-client python-dotenv langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzira-9BQq0h"
      },
      "source": [
        "## Setup Source Gitlab with PyAirbyte\n",
        "\n",
        "The provided code configures an Airbyte source to extract data from a gitlab.\n",
        "\n",
        "To configure according to your requirements, you can refer to [this references](https://docs.airbyte.com/integrations/sources/gitlab#reference).\n",
        "\n",
        "Note: The credentials are retrieved securely using the get_secret() method. This will automatically locate a matching Google Colab secret or environment variable, ensuring they are not hard-coded into the notebook. Make sure to add your key to the Secrets section on the left.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2pgyG5aMGq0"
      },
      "outputs": [],
      "source": [
        "import airbyte as ab\n",
        "\n",
        "source = ab.get_source(\n",
        "    \"source-gitlab\",\n",
        "    config={\n",
        "        \"credentials\":{\n",
        "          \"auth_type\":\"access_token\",\n",
        "          \"access_token\": ab.get_secret(\"GITLAB_ACCESS_TOKEN\"),\n",
        "        },\n",
        "        \"projects\" :ab.get_secret(\"GITLAB_PROJECT\")\n",
        "    }\n",
        ")\n",
        "source.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Phdo7l_MGq2"
      },
      "outputs": [],
      "source": [
        "# In this notebook we are focused on only issues stream\n",
        "# checkout all stream here : https://docs.airbyte.com/integrations/sources/gitlab#supported-streams\n",
        "\n",
        "source.get_available_streams()\n",
        "source.select_streams([\"issues\"])\n",
        "cache = ab.get_default_cache()\n",
        "result = source.read(cache=cache)\n",
        "\n",
        "issues_details = [doc for doc in result[\"issues\"].to_documents()]  # Fetching data for issues stream only\n",
        "\n",
        "print(str(issues_details[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ5Na_O2Sn1U"
      },
      "source": [
        "# Use Langchain to build a RAG pipeline.\n",
        "\n",
        "The code uses RecursiveCharacterTextSplitter to break documents into smaller chunks. Metadata within these chunks is converted to strings. This facilitates efficient processing of large texts, enhancing analysis capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wravAgJhMGq3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.split_documents(issues_details)\n",
        "print(f\"Created {len(chunked_docs)} document chunks.\")\n",
        "\n",
        "for doc in chunked_docs:\n",
        "    for md in doc.metadata:\n",
        "        doc.metadata[md] = str(doc.metadata[md])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tcXR48fsMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "## Embedding Technique Of OPENAI\n",
        "os.environ['OPENAI_API_KEY'] = ab.get_secret(\"OPENAI_API_KEY\")\n",
        "embeddings=OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh_lGwiJUkLg"
      },
      "source": [
        "## Setting up Qdrant\n",
        "\n",
        "Qdrant is leading open source vector database and similarity search engine designed to handle high-dimensional vectors for performance and massive-scale AI applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZXgQF-xMGq4"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "client = QdrantClient(\n",
        "    location=ab.get_secret(\"QDRANT_URL\"), # As obtain above\n",
        "    api_key=ab.get_secret(\"QDRANT_API_KEY\"),\n",
        ")\n",
        "\n",
        "collection_name = \"gitlab_issue\" # Give collection a name\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=1536, # vector dimensions\n",
        "        distance=models.Distance.COSINE,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU9KjhMHMGq4"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores.qdrant import Qdrant\n",
        "\n",
        "qdrant = Qdrant(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "\n",
        "qdrant.add_documents(chunked_docs, batch_size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zwaoOteU7oi"
      },
      "source": [
        "Now setting up a pipeline for RAG using LangChain, incorporating document retrieval from Pinecone, prompt configuration, and a chat model from OpenAI for response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y2e-raMYMGq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418f679a-dafe-4f21-d443-54e73e8c1fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langchain RAG pipeline set up successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = qdrant.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"Langchain RAG pipeline set up successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ihbo8bllMGq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feec51cc-2cfd-4634-e075-baa571b52b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The programming languages mentioned in the context are Java and JavaScript.\n"
          ]
        }
      ],
      "source": [
        "print(rag_chain.invoke(\"Which programing languages are mentioned in issues most?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}