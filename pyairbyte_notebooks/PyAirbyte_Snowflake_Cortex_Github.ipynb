{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3fUVL_19YGb"
      },
      "source": [
        "# Storing vector data into Snowflake using PyAirbyte, Snowflake Cortex\n",
        "\n",
        "In this notebook, we'll illustrate how to load data from airbyte-source into Snowflake using PyAirbyte, and afterwards convert the stream data into vector. In this, we've used source-github and stream 'issues', but you can replace the source according to your requirements.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **GitHub Access Token**:\n",
        "   - Follow the instructions in the [Github Connector Docs](https://docs.airbyte.com/integrations/sources/github) to set up your github and get api_token.\n",
        "\n",
        "2. **Snowflake**:\n",
        "   - To set up snowflake, follow these [instructions](https://docs.airbyte.com/integrations/destinations/snowflake#login-and-password).\n",
        "\n",
        "\n",
        "## Install PyAirbyte and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhMxXpazUzBX"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# First, we need to install the necessary libraries.\n",
        "!pip3 install airbyte snowflake-connector-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqfoQD2dAJtd"
      },
      "source": [
        "# Setup Source Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwV29bK4VbL4"
      },
      "outputs": [],
      "source": [
        "import airbyte as ab\n",
        "\n",
        "source = ab.get_source(\n",
        "    \"source-github\",\n",
        "    config={\n",
        "        \"repositories\": [\"airbytehq/quickstarts\"],\n",
        "        \"credentials\": {\n",
        "            \"personal_access_token\": ab.get_secret(\"GITHUB_API_KEY\"),\n",
        "        },\n",
        "    },\n",
        ")\n",
        "source.check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9avgcWxs9Ura"
      },
      "source": [
        "Reads the data from the selected issues stream, extracting the GitHub issues data for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8VFZEK8XbA0"
      },
      "outputs": [],
      "source": [
        "source.get_available_streams()\n",
        "source.select_streams([\"issues\"]) # we are only interested in issues stream\n",
        "read_result = source.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzeTFZ58YVEo"
      },
      "outputs": [],
      "source": [
        "issues = [doc for doc in read_result[\"issues\"].to_documents()]  # Will be useful for vector_embedding\n",
        "issue_df = read_result['issues'].to_pandas() # Converting data to pandas frame\n",
        "print(str(issues[5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fivYEfdBafd"
      },
      "source": [
        "# Loading data into Snowflake\n",
        "It uses the snowflake.connector module to connect to Snowflake with the provided credentials fetched from secrets, Make sure to add your key to the Secrets section on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EM-oHR3cXXc"
      },
      "outputs": [],
      "source": [
        "from snowflake import connector\n",
        "conn = connector.connect(\n",
        "        account=ab.get_secret(\"SNOWFLAKE_HOST\"),\n",
        "        role=ab.get_secret(\"SNOWFLAKE_ROLE\"),\n",
        "        warehouse=ab.get_secret(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "        database=ab.get_secret(\"SNOWFLAKE_DATABASE\"),\n",
        "        schema=ab.get_secret(\"SNOWFLAKE_SCHEMA\"),\n",
        "        user=ab.get_secret(\"SNOWFLAKE_USERNAME\"),\n",
        "        password=ab.get_secret(\"SNOWFLAKE_PASSWORD\"),\n",
        "    )\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(ab.get_secret(\"SNOWFLAKE_SCHEMA\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85uLgQ7JBgoI"
      },
      "source": [
        "A function to create a Snowflake table based on the schema of a Pandas DataFrame and then uses this function to create a github_issue table in Snowflake from the issue_df DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X96O3Is-i3v4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_table_from_dataframe(conn, df, table_name):\n",
        "    cursor = conn.cursor()\n",
        "    database = ab.get_secret('SNOWFLAKE_DATABASE')\n",
        "    print(database)\n",
        "    cursor.execute(f'USE DATABASE {database}')\n",
        "    schema_name = ab.get_secret('SNOWFLAKE_SCHEMA')\n",
        "    cursor.execute(f'USE SCHEMA {schema_name}')\n",
        "    columns = []\n",
        "    for column, dtype in zip(df.columns, df.dtypes):\n",
        "        if pd.api.types.is_integer_dtype(dtype):\n",
        "            snowflake_type = 'INTEGER'\n",
        "        elif pd.api.types.is_float_dtype(dtype):\n",
        "            snowflake_type = 'FLOAT'\n",
        "        elif pd.api.types.is_bool_dtype(dtype):\n",
        "            snowflake_type = 'BOOLEAN'\n",
        "        elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
        "            snowflake_type = 'TIMESTAMP'\n",
        "        else:\n",
        "            snowflake_type = 'STRING'\n",
        "\n",
        "        columns.append(f'\"{column}\" {snowflake_type}')\n",
        "\n",
        "    create_table_sql = f'CREATE TABLE {table_name} ({\", \".join(columns)});'\n",
        "    cursor.execute(create_table_sql)\n",
        "\n",
        "# Example usage:\n",
        "create_table_from_dataframe(conn, issue_df, 'github_issue') # Keep table name according to your requirments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RePkO21hCSZx"
      },
      "source": [
        "upload_dataframe_to_snowflake that uses Snowflake's pandas integration (write_pandas) to upload a Pandas DataFrame (issue_df) into a Snowflake table ('GITHUB_ISSUE')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku61-vxBt7Xh"
      },
      "outputs": [],
      "source": [
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "def upload_dataframe_to_snowflake(conn, df, table_name):\n",
        "    success, nchunks, nrows, _ = write_pandas(conn, df, table_name)\n",
        "    if success:\n",
        "        print(f\"Successfully inserted {nrows} rows into {table_name}.\")\n",
        "    else:\n",
        "        print(\"Failed to insert data.\")\n",
        "\n",
        "upload_dataframe_to_snowflake(conn, issue_df, 'GITHUB_ISSUE') # Remember to use table name in uppercase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppxeChRQCmm8"
      },
      "source": [
        "# Vector Embedding the Data\n",
        "Now we utilize the RecursiveCharacterTextSplitter from langchain.text_splitter to segment documents (issues) into smaller chunks based on specified parameters (chunk_size and chunk_overlap).\n",
        "\n",
        "Then we organize the chunked documents into a Pandas DataFrame (df) with columns for page content (PAGE_CONTENT), metadata (META), and type (TYPE), ensuring all data is represented as strings for consistency and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ti5uUpsu2oU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.split_documents(issues)\n",
        "print(f\"Created {len(chunked_docs)} document chunks.\")\n",
        "\n",
        "\n",
        "for doc in chunked_docs:\n",
        "    for md in doc.metadata:\n",
        "        doc.metadata[md] = str(doc.metadata[md])\n",
        "\n",
        "df = pd.DataFrame(chunked_docs, columns=['PAGE_CONTENT','META','TYPE']) # please use uppercase\n",
        "# Convert all columns to string\n",
        "df = df.astype(str)\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfSflgGwDkjN"
      },
      "source": [
        "Now we establish a new table for storing vector embedded data. First, we create a data and store chunked data, and then we vector embed the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFK12XSiNej_"
      },
      "outputs": [],
      "source": [
        "create_table_from_dataframe(conn, df, 'vector_github_issues')\n",
        "upload_dataframe_to_snowflake(conn, df, 'VECTOR_GITHUB_ISSUES') #use uppercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wjq35UMECIp"
      },
      "source": [
        "Now, using Snowflake Cortex, we will turn the page content column into embedding and store them in the embedding column. Different embedding models are available [here](https://docs.snowflake.com/en/sql-reference/functions/embed_text_1024-snowflake-cortex)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHHZ5fn7Rcs2"
      },
      "outputs": [],
      "source": [
        "\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Step 1: Add the new column to store the embeddings\n",
        "\n",
        "# We are using vector dimension 1024\n",
        "alter_table_query = \"\"\"\n",
        "ALTER TABLE VECTOR_GITHUB_ISSUES\n",
        "ADD COLUMN embedding VECTOR(FLOAT, 1024);\n",
        "\"\"\"\n",
        "cur.execute(alter_table_query)\n",
        "\n",
        "# Step 2: Update the new column with embeddings from Cortex\n",
        "# Note: Using a subquery to avoid issues with updating the same table in place\n",
        "update_query = \"\"\"\n",
        "UPDATE VECTOR_GITHUB_ISSUES\n",
        "SET embedding = (\n",
        "    SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_1024('nv-embed-qa-4', page_content)\n",
        ");\n",
        "\"\"\"\n",
        "cur.execute(update_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uyFarH2Fjvf"
      },
      "source": [
        "This approach demonstrates how to seamlessly integrate data retrieval from an Airbyte source, such as GitHub issues, and efficiently store it in Snowflake for further analysis. By utilizing PyAirbyte for data extraction and Snowflake's capabilities for data warehousing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
