{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End RAG Tutorial Using Github, PyAirbyte, Chroma, and LangChain**\n",
        "This notebook illustrates the complete setup of a Retrieval-Augmented Generation (RAG) pipeline.<br>\n",
        "We extract data from a GitHub repository using PyAirbyte, store the data in a Chroma vector store, and use LangChain to perform RAG on the stored data.<br>\n",
        "## **Prerequisites**\n",
        "**1) OpenAI API Key**:\n",
        "   - **Create an OpenAI Account**: Sign up for an account on [OpenAI](https://www.openai.com/).\n",
        "   - **Generate an API Key**: Go to the API section and generate a new API key. For detailed instructions, refer to the [OpenAI documentation](https://beta.openai.com/docs/quickstart).\n",
        "\n",
        "**2) Github Personal Access Token**:\n",
        "   - **Create a Github Account**: Sign up for an account on [Github](https://www.github.com/).\n",
        "   - **Generate an API Key**: Cick on your profile icon->Settings->Developer Settings and generate a new API key. For detailed instructions, refer to the [Github documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).\n"
      ],
      "metadata": {
        "id": "lv1fce-k_Ul5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Dependencies**\n",
        "First Thing First !<br>\n",
        "Lets get the dependencies installed before anything else!!"
      ],
      "metadata": {
        "id": "mbc7jQsW-kGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5FFGW2iOhOqT"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support for running in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# First, we need to install the necessary libraries.\n",
        "!pip3 install airbyte langchain langchain-openai chromadb python-dotenv langchainhub langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Source Setup : Github with PyAirbyte**\n",
        "The code you see below configures an Airbyte source to pull out data from a github repository.\n",
        "\n",
        "You can also customize the configuration according to your own needs.\n",
        "See [this](https://docs.airbyte.com/integrations/sources/github#reference)\n",
        "\n",
        "Note that we here only fetch data from the Commits Stream <br>\n",
        "To know about all the available streams go [here](https://docs.airbyte.com/integrations/sources/github#supported-streams)\n"
      ],
      "metadata": {
        "id": "aBfzg42A_mhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import airbyte as ab\n",
        "\n",
        "source = ab.get_source(\n",
        "    \"source-github\",\n",
        "    config={\n",
        "        \"credentials\": {\n",
        "            \"personal_access_token\": \"your_personal_access_token\"\n",
        "        },\n",
        "        \"repositories\": [\"your_github_username/your_repository_ID\"]\n",
        "    }\n",
        ")\n",
        "source.check()\n",
        "\n",
        "source.get_available_streams()\n",
        "source.select_streams([\"commits\"])\n",
        "cache = ab.get_default_cache()\n",
        "result = source.read(cache=cache)\n",
        "\n",
        "commits_details = [doc for doc in result[\"commits\"].to_documents()]\n",
        "\n",
        "print(str(commits_details[0]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vMiRDXs1ii3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Documents into Chunks**\n",
        "Large documents are split into smaller chunks to make them easier to handle. This also helps in improving the efficiency of the retrieval process, as smaller chunks can be more relevant to specific queries.\n",
        "<br>\n",
        "Here we set each chunk size to 512 characters and adjacent chunks will overlap by 50 characters to ensure continuity of context\n",
        "<br>\n",
        "Then the loop converts all metadata to string format to ensure consistent processing later in the pipeline."
      ],
      "metadata": {
        "id": "f4yYrQ9lLVRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "chunked_docs = splitter.split_documents(commits_details)\n",
        "\n",
        "for doc in chunked_docs:\n",
        "    for md in doc.metadata:\n",
        "        doc.metadata[md] = str(doc.metadata[md])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_shmtR8Zl6d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ab.get_secret(\"YOUR_OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CNgHPzULmB0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting up Chroma**\n",
        "Create and configure a Chroma vector store to store the document embeddings.<br>\n",
        "First we initialize Chroma Client\n",
        "<br>\n",
        "Then we create Chroma Vector Store from Documents\n",
        "<br>\n",
        "Finally we use embedding function when accessing the collection\n",
        "<br>\n",
        "Since currently there is a waitlist for Chroma,We initialize the Chroma client in persistent mode (local file)"
      ],
      "metadata": {
        "id": "TwHTBa43M7fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain_chroma import Chroma\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "persist_directory = 'chroma_db'\n",
        "client = chromadb.PersistentClient(path=persist_directory)\n",
        "collection_name = \"github_commits\"\n",
        "\n",
        "openai_lc_client = Chroma.from_documents(\n",
        "    documents=chunked_docs,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    model_name=\"text-embedding-ada-002\"\n",
        ")\n",
        "collection = client.get_collection(name=collection_name, embedding_function=openai_ef)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PzB1caQTmbuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Querying Chroma and RAG Pipeline**\n",
        "Finally we use LangChain to retrieve documents from Chroma and generate responses using an OpenAI chat model."
      ],
      "metadata": {
        "id": "MoCApKIWNpBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Set up the retriever from the Chroma vector store\n",
        "retriever = openai_lc_client.as_retriever()\n",
        "\n",
        "# Set up the prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Function to format documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Langchain RAG pipeline set up successfully.\")\n",
        "\n",
        "# Example query\n",
        "response = rag_chain.invoke(\"Which are the commit messages of latest commits?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "uDPPklkxod8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}