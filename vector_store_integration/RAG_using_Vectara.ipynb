{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayf6NOyRb-zn"
      },
      "source": [
        "# **Airbyte Vectara RAG Tutorial**\n",
        "This tutorial showcases how to use data stored in Airbyte's Vectara destination to perform Retrieval-Augmented Generation (RAG).\n",
        "## **Prerequisites**\n",
        "**1) OpenAI API Key**:\n",
        "   - **Create an OpenAI Account**: Sign up for an account on [OpenAI](https://www.openai.com/).\n",
        "   - **Generate an API Key**: Go to the API section and generate a new API key. For detailed instructions, refer to the [OpenAI documentation](https://beta.openai.com/docs/quickstart).\n",
        "\n",
        "**2) Vectara Customer ID, Corpus ID ,API Key**:\n",
        "   - **Create an Vectara Account**: Sign up for an account on [Vectara](https://vectara.com/).\n",
        "   - **Customer ID**: Click on the profile icon on top right, and look for your customer ID [Vectara Console](https://console.vectara.com/).\n",
        "   - **Corpus ID**: You can see the list of Corpora you've created in your Vectara Account. Note down the required Corpus ID [Vectara Corpora](https://console.vectara.com/console/corpora).\n",
        "   - **Generate an API Key**: Go here and generate a new API key. [Vectara API_Key](https://console.vectara.com/console/apiAccess/personalApiKey)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41LFzVLEmXT3"
      },
      "source": [
        "# **Install Dependencies**\n",
        "As in any other Python Code, the first step is to install the required Dependencies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSOZnJ6G9zQu"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support in Google Colab\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# Install required packages\n",
        "%pip install --quiet openai langchain-openai tiktoken pandas langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQTCEicCmx2A"
      },
      "source": [
        "# **Set Up Environment Variables**\n",
        "We configure the required credentials here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94bGodjCFfsN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"VECTARA_CUSTOMER_ID\"] = \"YOUR_VECTARA_CUSTOMER_ID\"\n",
        "os.environ[\"VECTARA_CORPUS_ID\"] = \"YOUR_VECTARA_CORPUS_ID\"\n",
        "os.environ[\"VECTARA_API_KEY\"] = \"YOUR_VECTARA_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXR8wOMWnztR"
      },
      "source": [
        "# **Initialize Vectara Vector Store**\n",
        "We start by initializing the Vectara Vector Store and then see what the data in Vectara looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipg7aPJq-wZ8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain_community.vectorstores import Vectara\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize Vectara vector store\n",
        "vectara = Vectara(\n",
        "    vectara_customer_id=os.getenv(\"VECTARA_CUSTOMER_ID\"),\n",
        "    vectara_corpus_id=os.getenv(\"VECTARA_CORPUS_ID\"),\n",
        "    vectara_api_key=os.getenv(\"VECTARA_API_KEY\")\n",
        ")\n",
        "\n",
        "def fetch_vectara_data():\n",
        "    # Simulate fetching data from Vectara\n",
        "    data = {\n",
        "        \"document_id\": [1, 2, 3],\n",
        "        \"document_content\": [\"Content of doc 1\", \"Content of doc 2\", \"Content of doc 3\"],\n",
        "        \"metadata\": [\"Metadata1\", \"Metadata2\", \"Metadata3\"],\n",
        "        \"embedding\": [\"[0.1, 0.2, ...]\", \"[0.3, 0.4, ...]\", \"[0.5, 0.6, ...]\"]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# show data\n",
        "data_frame = fetch_vectara_data()\n",
        "print(data_frame)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b0VCg4Oox4l"
      },
      "source": [
        "# **Embedding and similarity search with Vectara**\n",
        "Here we will convert the user's query into embeddings using OpenAI and retrieve similar chunks from Vectara based on the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6UuEXP9FpdF"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "from rich.console import Console\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Convert user's query into a vector array to prep for similarity search\n",
        "def get_embedding_from_openai(query) -> List[float]:\n",
        "    print(f\"Embedding user's query -> {query}...\")\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=client.api_key)\n",
        "    return embeddings.embed_query(query)\n",
        "\n",
        "# Use Vectara to find matching chunks\n",
        "def get_similar_chunks_from_vectara(query: str) -> List[str]:\n",
        "    print(\"\\nRetrieving similar chunks...\")\n",
        "    try:\n",
        "        results = vectara.similarity_search(query=query)\n",
        "        chunks = [result.page_content for result in results]\n",
        "        print(f\"Found {len(chunks)} matching chunks!\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retrieving chunks: {e}\")\n",
        "        return []\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEQN6yohqVSY"
      },
      "source": [
        "# **Building RAG Pipeline and asking a question**\n",
        "Finally we use OpenAI for querying our data! <br>\n",
        "We know the three main steps of a RAG Pipeline are : <br>\n",
        "- Embedding incoming query <br>\n",
        "- Doing similarity search to find matching chunks <br>\n",
        "- Send chunks to LLM for completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLKAuK-GqLPC"
      },
      "outputs": [],
      "source": [
        "# Use OpenAI to complete the response\n",
        "def get_completion_from_openai(question, document_chunks: List[str], model_name=\"gpt-3.5-turbo\"):\n",
        "    print(f\"\\nSending chunks to OpenAI (LLM: {model_name}) for completion...\")\n",
        "    chunks = \"\\n\\n\".join(document_chunks)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an Airbyte product assistant. Answer the question based on the context. Do not use any other information. Be concise.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{chunks}\\n\\n{question}\\n\\nAnswer:\"}\n",
        "    ],\n",
        "    max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Putting it all together\n",
        "def get_response(query, model_name=\"gpt-3.5-turbo\"):\n",
        "\n",
        "    chunks = get_similar_chunks_from_vectara(query)\n",
        "\n",
        "    if len(chunks) == 0:\n",
        "        return \"I am sorry, I do not have the context to answer your question.\"\n",
        "    else:\n",
        "\n",
        "        return get_completion_from_openai(query, chunks, model_name)\n",
        "\n",
        "# Ask a question\n",
        "query = 'What data do you have?'\n",
        "response = get_response(query)\n",
        "\n",
        "Console().print(f\"\\n\\nResponse from LLM:\\n\\n[blue]{response}[/blue]\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
